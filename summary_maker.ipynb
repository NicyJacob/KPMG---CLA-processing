{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import PyPDF2\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import heapq\n",
    "import pandas as pd\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf2info(pdf):\n",
    "    ''' Function to convert pdf to cleaned text '''\n",
    "    try:\n",
    "        \n",
    "        #Now give the pdf name\n",
    "        pdf = pdf\n",
    "        pdfFileObj = open(pdf, 'rb')\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        #print(pdfReader.numPages) # will give total number of pages in pdf\n",
    "        text_extract = []\n",
    "        for i in range(pdfReader.numPages):\n",
    "            pageObj = pdfReader.getPage(i)\n",
    "            text=(pageObj.extractText())\n",
    "            text=text.split(\",\")\n",
    "            text_extract.append(text)\n",
    "\n",
    "        x = format_text(text_extract)\n",
    "        return(x)\n",
    "    except IOError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text(text_extract):\n",
    "    '''Function to clean text'''\n",
    "    text = text_extract\n",
    "    formatted_text = \"\"\n",
    "    for txt in text:\n",
    "        for t in txt:\n",
    "            txt1 = re.sub(r\"\\n+\", \" \", t)\n",
    "            formatted_text += txt1\n",
    "    return(formatted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(\"C:/Users/nicyj/Downloads/NL_337-2021-014822.txt\")\n",
    "text_extract = file1.read()\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_text = format_text(text_extract)\n",
    "formattedtext = str(formatted_text)\n",
    "print(type(formattedtext))\n",
    "\n",
    "sentences = nltk.sent_tokenize(formattedtext)\n",
    "stopwords = nltk.corpus.stopwords.words('dutch')\n",
    "#print(stopwords)\n",
    "word_frequencies = {}\n",
    "for word in nltk.word_tokenize(formatted_text):\n",
    "    if word not in stopwords:\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1\n",
    "    maximum_frequncy = max(word_frequencies.values())\n",
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "    sentence_scores = {}\n",
    "    for sent in sentences:\n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                if len(sent.split(' ')) < 30:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word]\n",
    "import heapq\n",
    "summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "summary = ' '.join(summary_sentences)\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8d57a0f13d636405d6879443318fafb29486b6db7cd47c7617fe26b594f1783"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
