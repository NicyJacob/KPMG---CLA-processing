{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "339d0a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import PyPDF2\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6ffd906",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pdf2info(pdf):\n",
    "    ''' Function to convert pdf to cleaned text '''\n",
    "    try:\n",
    "        \n",
    "        #Now give the pdf name\n",
    "        pdf = pdf\n",
    "        pdfFileObj = open(pdf, 'rb')\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        #print(pdfReader.numPages) # will give total number of pages in pdf\n",
    "        text_extract = []\n",
    "        for i in range(pdfReader.numPages):\n",
    "            pageObj = pdfReader.getPage(i)\n",
    "            text=(pageObj.extractText())\n",
    "            text=text.split(\",\")\n",
    "            text_extract.append(text)\n",
    "\n",
    "        x = format_text(text_extract)\n",
    "        return(x)\n",
    "    except IOError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5120a573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text(text_extract):\n",
    "    '''Function to clean text'''\n",
    "    text = text_extract\n",
    "    formatted_text = \"\"\n",
    "    for txt in text:\n",
    "        for t in txt:\n",
    "            txt1 = re.sub(r\"\\n+\", \" \", t)\n",
    "            formatted_text += txt1\n",
    "    return(formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38eded24",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(\"C:/Users/nicyj/Downloads/NL_337-2021-014822.txt\")\n",
    "text_extract = file1.read()\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45396c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Op deze bijkomende vergoeding worden, voor wat deze collectieve arbeidsovereenkomst betreft, desgevallend de wettelijke afhoudingen verricht en zij zijn steeds ten laste van de werknemer. Deze stekel; van werkloosheid met bedrijfstoeslag geldt voor werknemers die, rekening houdend met de in de collectieve arbeidsovereenkomst nr. Het laatste brutomaandloon, berekend en geplafonneerd volgens de bepalingen voorzien in de collectieve arbeidsovereenkomst nr. Deze collectieve arbeidsovereenkomst wordt uitdrukkelijk gesloten in uitvoering van: 1Â° de collectieve arbeidsovereenkomst nr. 17 van de Nationale Arbeidsraad voorziene overlegprocedure, worden ontslagen tijdens de geldigheidsduur van deze overeenkomst, behalve om dringende redenen. Echter kan in uitvoering van artikel 9, Â§1 van het koninklijk besluit van 3 mei 2007 vrijstelling van de vervangingsplicht toegestaan worden door de directeur van het bevoegde werkloosheidsbureau. De aanvullende vergoeding wordt aan de betrokken werknemers maandelijks betaald tot zij de wettelijke pensioenleeftijd hebben bereikt, tenzij de werknemer voor die tijd zou overlijden.\n"
     ]
    }
   ],
   "source": [
    "formatted_text = format_text(text_extract)\n",
    "\n",
    "#print(formatted_text)\n",
    "\n",
    "sentences = nltk.sent_tokenize(formatted_text)\n",
    "stopwords = nltk.corpus.stopwords.words('dutch')\n",
    "#print(stopwords)\n",
    "word_frequencies = {}\n",
    "for word in nltk.word_tokenize(formatted_text):\n",
    "    if word not in stopwords:\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1\n",
    "    maximum_frequncy = max(word_frequencies.values())\n",
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "    sentence_scores = {}\n",
    "    for sent in sentences:\n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                if len(sent.split(' ')) < 30:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word]\n",
    "import heapq\n",
    "summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "summary = ' '.join(summary_sentences)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b3666a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cla",
   "language": "python",
   "name": "cla"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
